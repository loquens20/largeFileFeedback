#!/usr/bin/env python3
"""
ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ë°ëª¨
API í´ë¼ì´ì–¸íŠ¸ ì—†ì´ë„ ì‹œìŠ¤í…œ ë™ì‘ í™•ì¸ ê°€ëŠ¥
"""

import os
import time
from pathlib import Path
from integrated_processor import IntegratedProcessor
from llm_large_file_processor import ModelPricing


def create_demo_file(size_mb: int = 35) -> str:
    """ë°ëª¨ìš© ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"""
    demo_file = "demo_large_file.txt"

    if Path(demo_file).exists():
        print(f"âœ“ ê¸°ì¡´ ë°ëª¨ íŒŒì¼ ì‚¬ìš©: {demo_file}")
        return demo_file

    print(f"ğŸ“ {size_mb}MB ë°ëª¨ íŒŒì¼ ìƒì„± ì¤‘...")

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸
    sample_text = """
ì´ê²ƒì€ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ìƒ˜í”Œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì—°êµ¬ ë…¼ë¬¸, ê¸°ìˆ  ë¬¸ì„œ, ê³„ì•½ì„œ ë“±ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì£¼ìš” íŠ¹ì§•:
1. 31MBë¥¼ ì´ˆê³¼í•˜ëŠ” íŒŒì¼ ì²˜ë¦¬
2. ì¼ì‹œì •ì§€ ë° ì¬ê°œ ê¸°ëŠ¥
3. ì‹¤ì‹œê°„ ë¹„ìš© ì¶”ì 
4. ëª¨ë¸ ë³€ê²½ ì§€ì›

ì´ ì‹œìŠ¤í…œì€ ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤:
- Microsoft Word (.docx)
- PDF ë¬¸ì„œ (.pdf)
- PowerPoint (.pptx)
- Excel (.xlsx)
- ì¼ë°˜ í…ìŠ¤íŠ¸ (.txt)

""" * 100

    target_size = size_mb * 1024 * 1024
    current_size = 0

    with open(demo_file, 'w', encoding='utf-8') as f:
        while current_size < target_size:
            f.write(sample_text)
            current_size += len(sample_text.encode('utf-8'))

    actual_size = os.path.getsize(demo_file) / (1024 * 1024)
    print(f"âœ“ ë°ëª¨ íŒŒì¼ ìƒì„± ì™„ë£Œ: {actual_size:.2f}MB")

    return demo_file


def demo_basic_processing():
    """ê¸°ë³¸ ì²˜ë¦¬ ë°ëª¨"""
    print("\n" + "="*70)
    print("ë°ëª¨ 1: ê¸°ë³¸ íŒŒì¼ ì²˜ë¦¬")
    print("="*70)

    # ë°ëª¨ íŒŒì¼ ìƒì„±
    demo_file = create_demo_file(35)

    # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” (API í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ = ë°ëª¨ ëª¨ë“œ)
    processor = IntegratedProcessor(api_client=None)

    print("\nğŸ“Š ë¹„ìš© ì˜ˆì¸¡ ì¤‘...")

    # ì²­í¬ ìƒì„±
    chunks = processor.preprocess_and_chunk(demo_file, chunk_size=100000)

    # ë¹„ìš© ì˜ˆì¸¡
    total_input_tokens = sum(chunk.token_estimate for chunk in chunks)
    output_per_chunk = 1000
    total_output_tokens = len(chunks) * output_per_chunk

    print(f"\níŒŒì¼ ì •ë³´:")
    print(f"  - íŒŒì¼ í¬ê¸°: {os.path.getsize(demo_file) / (1024*1024):.2f}MB")
    print(f"  - ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
    print(f"  - ì˜ˆìƒ ì…ë ¥ í† í°: {total_input_tokens:,}")
    print(f"  - ì˜ˆìƒ ì¶œë ¥ í† í°: {total_output_tokens:,}")

    print(f"\nğŸ’° ëª¨ë¸ë³„ ì˜ˆìƒ ë¹„ìš©:")
    costs = ModelPricing.compare_models(total_input_tokens, total_output_tokens)

    for model, cost in sorted(costs.items(), key=lambda x: x[1]):
        savings = costs['claude-opus-4'] - cost if model != 'claude-opus-4' else 0
        savings_pct = (savings / costs['claude-opus-4'] * 100) if savings > 0 else 0

        print(f"  {model:25s}: ${cost:8.2f}", end="")
        if savings > 0:
            print(f"  (ğŸ’° ${savings:.2f} ì ˆì•½, {savings_pct:.1f}%)", end="")
        print()

    # ì²˜ë¦¬ ì‹œì‘ (ë°ëª¨ ëª¨ë“œ)
    print(f"\nâ–¶ï¸  ì²˜ë¦¬ ì‹œì‘ (ë°ëª¨ ëª¨ë“œ - ì‹¤ì œ API í˜¸ì¶œ ì—†ìŒ)")
    print("   Ctrl+Cë¥¼ ëˆŒëŸ¬ ì¼ì‹œì •ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    state = processor.process_file(
        file_path=demo_file,
        system_prompt="ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€",
        user_prompt_template="ë‹¤ìŒ ë‚´ìš©ì„ ìš”ì•½: {chunk_text}",
        model="claude-haiku-4",
        output_tokens=1000,
        auto_confirm=True  # ë°ëª¨ì—ì„œëŠ” ìë™ í™•ì¸
    )

    print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   ì´ ë¹„ìš© (ë°ëª¨): ${state.total_cost:.4f}")
    print(f"   ì²˜ë¦¬ëœ ì²­í¬: {state.processed_chunks}/{state.total_chunks}")


def demo_pause_resume():
    """ì¼ì‹œì •ì§€/ì¬ê°œ ë°ëª¨"""
    print("\n" + "="*70)
    print("ë°ëª¨ 2: ì¼ì‹œì •ì§€ ë° ì¬ê°œ")
    print("="*70)

    demo_file = "demo_large_file.txt"

    if not Path(demo_file).exists():
        print("âŒ ë°ëª¨ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ëª¨ 1ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    processor = IntegratedProcessor(api_client=None)

    # ê¸°ì¡´ ìƒíƒœ í™•ì¸
    state = processor.file_processor.load_state(demo_file)

    if state is None:
        print("âŒ ì €ì¥ëœ ì²˜ë¦¬ ìƒíƒœê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € ë°ëª¨ 1ì„ ì‹¤í–‰í•˜ê³  Ctrl+Cë¡œ ì¤‘ë‹¨í•˜ì„¸ìš”.")
        return

    print(f"\nğŸ“‚ ì €ì¥ëœ ìƒíƒœ ë°œê²¬:")
    print(f"   ì§„í–‰: {state.processed_chunks}/{state.total_chunks} ì²­í¬")
    print(f"   ëª¨ë¸: {state.current_model}")
    print(f"   ëˆ„ì  ë¹„ìš©: ${state.total_cost:.4f}")

    # ëª¨ë¸ ë³€ê²½ ë°ëª¨
    print(f"\nğŸ”„ ëª¨ë¸ ë³€ê²½: {state.current_model} â†’ gpt-4o-mini")
    processor.file_processor.change_model("gpt-4o-mini")

    # ë‚¨ì€ ë¹„ìš© ì˜ˆì¸¡
    chunks = processor.preprocess_and_chunk(demo_file)
    remaining_cost = processor.file_processor.estimate_remaining_cost(
        chunks, state.processed_chunks, "gpt-4o-mini", 1000
    )

    print(f"\nğŸ’° ë‚¨ì€ ì²˜ë¦¬ ì˜ˆìƒ ë¹„ìš©:")
    print(f"   ë‚¨ì€ ì²­í¬: {remaining_cost['remaining_chunks']}")
    print(f"   ì˜ˆìƒ ë¹„ìš©: ${remaining_cost['estimated_cost']:.4f}")

    print(f"\nâ–¶ï¸  ì²˜ë¦¬ ì¬ê°œ...")

    # ì¬ê°œ
    processor._process_chunks(
        chunks=chunks,
        state=state,
        system_prompt="ë¬¸ì„œ ë¶„ì„",
        user_prompt_template="{chunk_text}",
        model="gpt-4o-mini",
        output_tokens=1000
    )

    print(f"\nâœ… ì™„ë£Œ!")


def demo_model_comparison():
    """ëª¨ë¸ ë¹„êµ ë°ëª¨"""
    print("\n" + "="*70)
    print("ë°ëª¨ 3: ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ë³„ ìµœì  ëª¨ë¸")
    print("="*70)

    scenarios = [
        {
            'name': 'ì†Œê·œëª¨ ìš”ì•½ (10MB, ê°„ë‹¨)',
            'input_tokens': 500_000,
            'output_tokens': 50_000
        },
        {
            'name': 'ì¤‘ê·œëª¨ ë²ˆì—­ (50MB, ì¤‘ê°„)',
            'input_tokens': 2_500_000,
            'output_tokens': 2_500_000
        },
        {
            'name': 'ëŒ€ê·œëª¨ ë¶„ì„ (100MB, ë³µì¡)',
            'input_tokens': 5_000_000,
            'output_tokens': 500_000
        }
    ]

    for scenario in scenarios:
        print(f"\nğŸ“Š {scenario['name']}")
        print(f"   ì…ë ¥: {scenario['input_tokens']:,} í† í°")
        print(f"   ì¶œë ¥: {scenario['output_tokens']:,} í† í°")
        print()

        costs = ModelPricing.compare_models(
            scenario['input_tokens'],
            scenario['output_tokens']
        )

        sorted_costs = sorted(costs.items(), key=lambda x: x[1])
        cheapest = sorted_costs[0]

        for model, cost in sorted_costs:
            marker = "ğŸ‘ˆ ì¶”ì²œ" if model == cheapest[0] else ""
            time_estimate = scenario['input_tokens'] / 1000  # ëŒ€ëµì  ì‹œê°„(ì´ˆ)

            print(f"   {model:25s}: ${cost:8.2f}  {marker}")

        print(f"\n   ğŸ’¡ ê¶Œì¥: {cheapest[0]} (${cheapest[1]:.2f})")


def demo_interactive():
    """ëŒ€í™”í˜• ë°ëª¨"""
    print("\n" + "="*70)
    print("ëŒ€í™”í˜• ë°ëª¨")
    print("="*70)

    while True:
        print("\në‹¤ìŒ ì¤‘ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ê¸°ë³¸ ì²˜ë¦¬ ë°ëª¨")
        print("2. ì¼ì‹œì •ì§€/ì¬ê°œ ë°ëª¨")
        print("3. ëª¨ë¸ ë¹„êµ")
        print("4. ì²˜ë¦¬ ìƒíƒœ í™•ì¸")
        print("5. ì¢…ë£Œ")

        choice = input("\nì„ íƒ (1-5): ").strip()

        if choice == '1':
            demo_basic_processing()
        elif choice == '2':
            demo_pause_resume()
        elif choice == '3':
            demo_model_comparison()
        elif choice == '4':
            processor = IntegratedProcessor()
            state_files = list(processor.file_processor.state_dir.glob("*.json"))

            if not state_files:
                print("\nì €ì¥ëœ ìƒíƒœê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print(f"\nì €ì¥ëœ ì²˜ë¦¬ ìƒíƒœ: {len(state_files)}ê°œ")
                for sf in state_files:
                    import json
                    with open(sf) as f:
                        data = json.load(f)
                    print(f"\n  ğŸ“„ {Path(data['file_path']).name}")
                    print(f"     ì§„í–‰: {data['processed_chunks']}/{data['total_chunks']}")
                    print(f"     ë¹„ìš©: ${data['total_cost']:.4f}")
        elif choice == '5':
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        else:
            print("\nâŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")


if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                              â•‘
â•‘      ëŒ€ìš©ëŸ‰ íŒŒì¼ LLM ì²˜ë¦¬ ì‹œìŠ¤í…œ - ë°ëª¨                       â•‘
â•‘                                                              â•‘
â•‘  â€¢ 31MB+ íŒŒì¼ ì²˜ë¦¬                                           â•‘
â•‘  â€¢ ì¼ì‹œì •ì§€/ì¬ê°œ (Ctrl+C)                                    â•‘
â•‘  â€¢ ë¹„ìš© ìµœì í™”                                               â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    print("\nì´ ë°ëª¨ëŠ” API í´ë¼ì´ì–¸íŠ¸ ì—†ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    print("ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ANTHROPIC_API_KEY ë˜ëŠ” OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

    demo_interactive()
