"""
í†µí•© LLM ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- ì „ì²˜ë¦¬ â†’ ì²­í‚¹ â†’ LLM ì²˜ë¦¬
- ì¼ì‹œì •ì§€/ì¬ê°œ ê¸°ëŠ¥
- ì‹¤ì‹œê°„ ë¹„ìš© ì¶”ì 
- ì§„í–‰ë¥  í‘œì‹œ
"""

import os
import json
import time
from typing import List, Dict, Any, Optional, Callable
from pathlib import Path
from tqdm import tqdm

from llm_large_file_processor import (
    LargeFileProcessor, ChunkInfo, ProcessingState, ModelPricing
)
from document_preprocessor import DocumentPreprocessor, ExtractedContent


class IntegratedProcessor:
    """í†µí•© ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""

    def __init__(self,
                 api_client: Any = None,  # LLM API í´ë¼ì´ì–¸íŠ¸
                 state_dir: str = "./processing_states",
                 chunk_dir: str = "./chunks"):
        """
        Args:
            api_client: LLM API í´ë¼ì´ì–¸íŠ¸ (ì˜ˆ: Anthropic, OpenAI)
            state_dir: ì§„í–‰ ìƒí™© ì €ì¥ ë””ë ‰í† ë¦¬
            chunk_dir: ì²­í¬ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.file_processor = LargeFileProcessor(state_dir)
        self.preprocessor = DocumentPreprocessor()
        self.api_client = api_client
        self.chunk_dir = Path(chunk_dir)
        self.chunk_dir.mkdir(exist_ok=True)

        self.pause_requested = False

    def preprocess_and_chunk(self,
                            file_path: str,
                            chunk_size: int = 80000,  # ~40k í† í°
                            overlap: int = 4000) -> List[ChunkInfo]:
        """
        1ë‹¨ê³„: íŒŒì¼ ì „ì²˜ë¦¬ ë° ì²­í‚¹
        - ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ì¶”ì¶œ
        - ì²­í¬ë¡œ ë¶„í• 
        - ë””ìŠ¤í¬ì— ì €ì¥
        """
        print("\n" + "="*60)
        print("1ë‹¨ê³„: íŒŒì¼ ì „ì²˜ë¦¬ ë° ì²­í‚¹")
        print("="*60)

        # íŒŒì¼ í•´ì‹œ ê³„ì‚°
        file_hash = self.file_processor.calculate_file_hash(file_path)
        chunk_file = self.chunk_dir / f"{file_hash}_chunks.json"

        # ê¸°ì¡´ ì²­í¬ í™•ì¸
        if chunk_file.exists():
            print(f"ğŸ“‚ ê¸°ì¡´ ì²­í¬ íŒŒì¼ ë°œê²¬: {chunk_file}")
            with open(chunk_file, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)

            chunks = [ChunkInfo(**c) for c in chunk_data]
            print(f"âœ“ {len(chunks)}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
            return chunks

        # ìƒˆë¡œ ì¶”ì¶œ
        print("\nğŸ“„ ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ ì¤‘...")
        extracted = self.preprocessor.extract_content(file_path)

        # ì²­í¬ ìƒì„±
        print("\nğŸ”¨ ì²­í¬ ìƒì„± ì¤‘...")
        chunks = self._create_smart_chunks(extracted, chunk_size, overlap)

        # ì²­í¬ ì €ì¥
        chunk_data = [
            {
                'index': c.index,
                'content_type': c.content_type,
                'text_content': c.text_content,
                'image_data': c.image_data,
                'token_estimate': c.token_estimate,
                'original_position': c.original_position
            }
            for c in chunks
        ]

        with open(chunk_file, 'w', encoding='utf-8') as f:
            json.dump(chunk_data, f, indent=2, ensure_ascii=False)

        print(f"âœ“ ì²­í¬ ì €ì¥ ì™„ë£Œ: {chunk_file}")
        print(f"   ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

        return chunks

    def _create_smart_chunks(self,
                            contents: List[ExtractedContent],
                            chunk_size: int,
                            overlap: int) -> List[ChunkInfo]:
        """ìŠ¤ë§ˆíŠ¸ ì²­í‚¹: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ì ì ˆíˆ ë¶„í• """
        chunks = []
        current_text = []
        current_images = []
        current_size = 0
        chunk_index = 0
        start_position = 0

        for content in contents:
            if content.content_type == 'text':
                text_size = len(content.data)

                # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ìƒˆ ì²­í¬ ìƒì„±
                if current_size + text_size > chunk_size and current_text:
                    chunks.append(self._finalize_chunk(
                        chunk_index, current_text, current_images, start_position
                    ))
                    chunk_index += 1

                    # ì˜¤ë²„ë© ì²˜ë¦¬
                    if current_text:
                        overlap_text = current_text[-1][-overlap:]
                        current_text = [overlap_text]
                        current_size = len(overlap_text)
                    else:
                        current_text = []
                        current_size = 0

                    current_images = []
                    start_position = content.position

                current_text.append(content.data)
                current_size += text_size

            elif content.content_type == 'image':
                current_images.append({
                    'position': content.position,
                    'data': content.data,
                    'format': 'png'
                })
                # ì´ë¯¸ì§€ëŠ” ì•½ 1500 í† í°ìœ¼ë¡œ ê³„ì‚°
                current_size += 3000  # ë¬¸ì ë‹¨ìœ„ë¡œ í™˜ì‚°

        # ë§ˆì§€ë§‰ ì²­í¬
        if current_text or current_images:
            chunks.append(self._finalize_chunk(
                chunk_index, current_text, current_images, start_position
            ))

        return chunks

    def _finalize_chunk(self,
                       index: int,
                       texts: List[str],
                       images: List[Dict],
                       start_position: int) -> ChunkInfo:
        """ì²­í¬ ì™„ì„±"""
        text_content = '\n\n'.join(texts) if texts else None
        image_data = images if images else None

        # ì½˜í…ì¸  íƒ€ì… ê²°ì •
        if text_content and image_data:
            content_type = 'mixed'
        elif image_data:
            content_type = 'image'
        else:
            content_type = 'text'

        # í† í° ì˜ˆì¸¡
        token_estimate = 0
        if text_content:
            token_estimate += len(text_content) // 2
        if image_data:
            token_estimate += len(image_data) * 1500

        return ChunkInfo(
            index=index,
            content_type=content_type,
            text_content=text_content,
            image_data=image_data,
            token_estimate=token_estimate,
            original_position=start_position
        )

    def process_file(self,
                    file_path: str,
                    system_prompt: str,
                    user_prompt_template: str,
                    model: str = 'claude-haiku-4',
                    output_tokens: int = 1000,
                    auto_confirm: bool = False) -> ProcessingState:
        """
        2ë‹¨ê³„: íŒŒì¼ ì²˜ë¦¬ ì‹¤í–‰

        Args:
            file_path: ì²˜ë¦¬í•  íŒŒì¼
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            user_prompt_template: ì²­í¬ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì˜ˆ: "{chunk_text}")
            model: ì‚¬ìš©í•  ëª¨ë¸
            output_tokens: ì˜ˆìƒ ì¶œë ¥ í† í° ìˆ˜
            auto_confirm: ë¹„ìš© í™•ì¸ ìë™ ìŠ¹ì¸
        """
        print("\n" + "="*60)
        print("2ë‹¨ê³„: LLM ì²˜ë¦¬ ì¤€ë¹„")
        print("="*60)

        # 1. ì „ì²˜ë¦¬ ë° ì²­í‚¹
        chunks = self.preprocess_and_chunk(file_path)

        # 2. ìƒíƒœ ì´ˆê¸°í™” ë˜ëŠ” ë¡œë“œ
        state = self.file_processor.initialize_processing(file_path, model)

        # 3. ë¹„ìš© ì˜ˆì¸¡
        cost_info = self.file_processor.estimate_remaining_cost(
            chunks, state.processed_chunks, model, output_tokens
        )

        print(f"\nğŸ’° ì˜ˆìƒ ë¹„ìš© (í˜„ì¬ ëª¨ë¸: {model})")
        print(f"   ë‚¨ì€ ì²­í¬: {cost_info['remaining_chunks']}ê°œ")
        print(f"   ì…ë ¥ í† í°: {cost_info['input_tokens']:,}")
        print(f"   ì¶œë ¥ í† í°: {cost_info['output_tokens']:,}")
        print(f"   ì˜ˆìƒ ë¹„ìš©: ${cost_info['estimated_cost']:.4f}")

        print(f"\nğŸ“Š ë‹¤ë¥¸ ëª¨ë¸ ë¹„ìš© ë¹„êµ:")
        for model_name, cost in sorted(cost_info['model_comparison'].items(),
                                       key=lambda x: x[1]):
            emoji = "ğŸ‘ˆ" if model_name == model else ""
            print(f"   {model_name:25s}: ${cost:8.4f} {emoji}")

        if not auto_confirm:
            print(f"\ní˜„ì¬ ì„ íƒ: {model} (${cost_info['estimated_cost']:.4f})")
            response = input("ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n/ëª¨ë¸ëª…): ").strip()

            if response.lower() == 'n':
                print("âŒ ì²˜ë¦¬ ì·¨ì†Œ")
                return state
            elif response.lower() != 'y':
                # ëª¨ë¸ ë³€ê²½
                if response in ModelPricing.MODELS:
                    self.file_processor.change_model(response)
                    model = response
                    # ë¹„ìš© ì¬ê³„ì‚°
                    cost_info = self.file_processor.estimate_remaining_cost(
                        chunks, state.processed_chunks, model, output_tokens
                    )
                    print(f"âœ“ ë³€ê²½ëœ ì˜ˆìƒ ë¹„ìš©: ${cost_info['estimated_cost']:.4f}")

        # 4. ì²˜ë¦¬ ì‹œì‘
        print("\n" + "="*60)
        print("3ë‹¨ê³„: LLM ì²˜ë¦¬ ì‹¤í–‰")
        print("="*60)

        self._process_chunks(
            chunks, state, system_prompt, user_prompt_template,
            model, output_tokens
        )

        return state

    def _process_chunks(self,
                       chunks: List[ChunkInfo],
                       state: ProcessingState,
                       system_prompt: str,
                       user_prompt_template: str,
                       model: str,
                       output_tokens: int):
        """ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬"""
        start_index = state.processed_chunks

        with tqdm(total=len(chunks), initial=start_index,
                 desc="Processing chunks") as pbar:

            for i in range(start_index, len(chunks)):
                if self.pause_requested:
                    print("\nâ¸ï¸  ì¼ì‹œì •ì§€ ìš”ì²­ë¨")
                    self.file_processor.save_state(state)
                    print("   ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    print("   ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œì‘í•˜ë ¤ë©´ ê°™ì€ íŒŒì¼ë¡œ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
                    break

                chunk = chunks[i]

                try:
                    # API í˜¸ì¶œ
                    result = self._call_llm_api(
                        chunk, system_prompt, user_prompt_template,
                        model, output_tokens
                    )

                    # ê²°ê³¼ ì €ì¥
                    state.results.append({
                        'chunk_index': i,
                        'response': result['response'],
                        'input_tokens': result['input_tokens'],
                        'output_tokens': result['output_tokens'],
                        'cost': result['cost']
                    })

                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    state.processed_chunks = i + 1
                    state.total_cost += result['cost']

                    # ì£¼ê¸°ì  ì €ì¥ (10ì²­í¬ë§ˆë‹¤)
                    if (i + 1) % 10 == 0:
                        self.file_processor.save_state(state)

                    pbar.set_postfix({
                        'cost': f"${state.total_cost:.4f}",
                        'chunk': f"{i+1}/{len(chunks)}"
                    })
                    pbar.update(1)

                except Exception as e:
                    print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ (ì²­í¬ {i}): {e}")
                    self.file_processor.save_state(state)
                    print("   ì§„í–‰ ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    raise

        # ìµœì¢… ì €ì¥
        self.file_processor.save_state(state)
        print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ì´ ë¹„ìš©: ${state.total_cost:.4f}")
        print(f"   ì²˜ë¦¬ëœ ì²­í¬: {state.processed_chunks}/{len(chunks)}")

    def _call_llm_api(self,
                     chunk: ChunkInfo,
                     system_prompt: str,
                     user_prompt_template: str,
                     model: str,
                     max_output_tokens: int) -> Dict[str, Any]:
        """
        ì‹¤ì œ LLM API í˜¸ì¶œ

        Note: ì´ ë©”ì„œë“œëŠ” ì‚¬ìš©í•˜ëŠ” APIì— ë§ê²Œ ìˆ˜ì • í•„ìš”
        """
        if self.api_client is None:
            # ë°ëª¨ ëª¨ë“œ: ì‹¤ì œ API í˜¸ì¶œ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜
            time.sleep(0.1)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
            return {
                'response': f"[Demo] Processed chunk {chunk.index}",
                'input_tokens': chunk.token_estimate,
                'output_tokens': 500,
                'cost': ModelPricing.estimate_cost(model, chunk.token_estimate, 500)
            }

        # ì‹¤ì œ API í˜¸ì¶œ ì˜ˆì‹œ (Anthropic Claude)
        # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        if chunk.text_content:
            content = user_prompt_template.format(chunk_text=chunk.text_content)
        else:
            content = "ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."

        messages = [{"role": "user", "content": content}]

        # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
        if chunk.image_data:
            image_contents = []
            for img in chunk.image_data:
                image_contents.append({
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/png",
                        "data": img['data']
                    }
                })
            # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë³´ëƒ„
            messages = [{
                "role": "user",
                "content": image_contents + [{"type": "text", "text": content}]
            }]

        try:
            # Anthropic API í˜¸ì¶œ
            if hasattr(self.api_client, 'messages'):
                response = self.api_client.messages.create(
                    model=model,
                    max_tokens=max_output_tokens,
                    system=system_prompt,
                    messages=messages
                )

                return {
                    'response': response.content[0].text,
                    'input_tokens': response.usage.input_tokens,
                    'output_tokens': response.usage.output_tokens,
                    'cost': ModelPricing.estimate_cost(
                        model,
                        response.usage.input_tokens,
                        response.usage.output_tokens
                    )
                }
            # OpenAI API í˜¸ì¶œ
            elif hasattr(self.api_client, 'chat'):
                response = self.api_client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        *messages
                    ],
                    max_tokens=max_output_tokens
                )

                return {
                    'response': response.choices[0].message.content,
                    'input_tokens': response.usage.prompt_tokens,
                    'output_tokens': response.usage.completion_tokens,
                    'cost': ModelPricing.estimate_cost(
                        model,
                        response.usage.prompt_tokens,
                        response.usage.completion_tokens
                    )
                }
        except Exception as e:
            print(f"API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            raise

    def request_pause(self):
        """ì¼ì‹œì •ì§€ ìš”ì²­"""
        self.pause_requested = True

    def export_results(self, state: ProcessingState, output_path: str):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        output_data = {
            'file': state.file_path,
            'model': state.current_model,
            'total_cost': state.total_cost,
            'processed_chunks': state.processed_chunks,
            'total_chunks': state.total_chunks,
            'created_at': state.created_at,
            'updated_at': state.updated_at,
            'results': state.results
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {output_path}")


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ëŒ€ìš©ëŸ‰ íŒŒì¼ LLM ì²˜ë¦¬ ì‹œìŠ¤í…œ                                  â•‘
â•‘  - 31MB+ íŒŒì¼ ì§€ì›                                           â•‘
â•‘  - ì¼ì‹œì •ì§€/ì¬ê°œ                                             â•‘
â•‘  - ì‹¤ì‹œê°„ ë¹„ìš© ì¶”ì                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # ë°ëª¨ ëª¨ë“œ (API í´ë¼ì´ì–¸íŠ¸ ì—†ì´)
    processor = IntegratedProcessor(api_client=None)

    print("\nì‚¬ìš© ë°©ë²•:")
    print("1. API í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ì˜ˆ: Anthropic, OpenAI)")
    print("2. processor.process_file() í˜¸ì¶œ")
    print("3. ì¼ì‹œì •ì§€: processor.request_pause()")
    print("4. ì¬ê°œ: ê°™ì€ íŒŒì¼ë¡œ ë‹¤ì‹œ ì‹¤í–‰")
    print("\ní•„ìš”í•œ íŒ¨í‚¤ì§€:")
    print("- python-docx, PyMuPDF, python-pptx, openpyxl")
    print("- anthropic ë˜ëŠ” openai (API í´ë¼ì´ì–¸íŠ¸)")
    print("- tqdm, Pillow")
